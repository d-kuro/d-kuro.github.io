<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | AFN Blog</title>
    <link>https://d-kuro.github.io/post/</link>
      <atom:link href="https://d-kuro.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 d-kuro</copyright><lastBuildDate>Wed, 06 May 2020 22:09:56 +0900</lastBuildDate>
    <image>
      <url>https://d-kuro.github.io/media/static/img/avatar.jpg</url>
      <title>Posts</title>
      <link>https://d-kuro.github.io/post/</link>
    </image>
    
    <item>
      <title>CIDR allocation failed for Kubernetes</title>
      <link>https://d-kuro.github.io/post/kubernetes-cidr/</link>
      <pubDate>Wed, 06 May 2020 22:09:56 +0900</pubDate>
      <guid>https://d-kuro.github.io/post/kubernetes-cidr/</guid>
      <description>&lt;p&gt;What to do when the Pod does not start due to CIDR allocation failure.&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;We are using 
&lt;a href=&#34;https://github.com/kubernetes-incubator/kube-aws&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kube-aws&lt;/a&gt; to build Kubernetes cluster.
One day, I saw that the Pod was in a lot of errors and would not start.
This has only occurred on certain nodes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;NAMESPACE     NAME                         READY   STATUS              RESTARTS   AGE
kube-system   canal-node-g2vfn             2/3     CrashLoopBackOff    20         79m
kube-system   kube-proxy-8n2lj             1/1     Running             0          79m
monitor       datadog-lr74g                0/3     Init:0/2            0          79m
production    foo-9d6944774-9jlls          0/2     ContainerCreating   0          55s
...
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;check-flannel-logs&#34;&gt;Check flannel logs&lt;/h2&gt;
&lt;p&gt;flannel had an error message like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;I0502 02:42:02.830771       1 main.go:514] Determining IP address of default interface
I0502 02:42:02.830960       1 main.go:527] Using interface with name eth0 and address 10.12.2.236
I0502 02:42:02.830972       1 main.go:544] Defaulting external address to interface address (10.12.2.236)
I0502 02:42:02.849385       1 kube.go:126] Waiting 10m0s for node controller to sync
I0502 02:42:02.849412       1 kube.go:309] Starting kube subnet manager
I0502 02:42:03.849506       1 kube.go:133] Node controller sync successful
I0502 02:42:03.849550       1 main.go:244] Created subnet manager: Kubernetes Subnet Manager - foo-node
I0502 02:42:03.849557       1 main.go:247] Installing signal handlers
I0502 02:42:03.849682       1 main.go:386] Found network config - Backend type: vxlan
I0502 02:42:03.849719       1 vxlan.go:120] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false
E0502 02:42:03.849902       1 main.go:289] Error registering network: failed to acquire lease: node &amp;quot;foo-node&amp;quot; pod cidr not assigned
I0502 02:42:03.849937       1 main.go:366] Stopping shutdownHandler...
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;check-controller-manager-logs&#34;&gt;Check controller-manager logs&lt;/h2&gt;
&lt;p&gt;controller-manager had an error message like the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;E0502 03:11:40.819569       1 controller_utils.go:282] Error while processing Node Add/Delete: failed to allocate cidr: CIDR allocation failed; there are no remaining CIDRs left to allocate in the accepted range
I0502 03:11:40.820095       1 event.go:221] Event(v1.ObjectReference{Kind:&amp;quot;Node&amp;quot;, Namespace:&amp;quot;&amp;quot;, Name:&amp;quot;foo-node&amp;quot;, UID:&amp;quot;2e10b7b3-8c14-11ea-a498-066c4ca4071a&amp;quot;, APIVersion:&amp;quot;&amp;quot;, ResourceVersion:&amp;quot;&amp;quot;, FieldPath:&amp;quot;&amp;quot;}): type: &#39;Normal&#39; reason: &#39;CIDRNotAvailable&#39; Node foo-node status is now: CIDRNotAvailable
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Kubernetes assigns a range of IP addresses (CIDR blocks) to each node so that a unique IP address is specified for each Pod.
By default, Kubernetes assigns /24 CIDR blocks (256 addresses) to each node.
This error message is displayed if the Node could not be assigned a CIDR blocks.&lt;/p&gt;
&lt;p&gt;This explanation is detailed in GKE documentation by GCP.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ“
&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optimizing IP address allocation Â |Â  Kubernetes Engine Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;check-cluster-cidr-blocks&#34;&gt;Check cluster CIDR blocks&lt;/h2&gt;
&lt;p&gt;Checking CIDR block for a cluster, run the following command on master node:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ps aux | grep cluster-cidr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run it, you will get the following results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root      3159  0.0  0.5 818604 89064 ?        Ssl  May02   1:53 /hyperkube controller-manager --cloud-provider=aws --cluster-name=foo-cluster --kubeconfig=/etc/kubernetes/kubeconfig/kube-controller-manager.yaml --authentication-kubeconfig=/etc/kubernetes/kubeconfig/kube-controller-manager.yaml --authorization-kubeconfig=/etc/kubernetes/kubeconfig/kube-controller-manager.yaml --leader-elect=true --root-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-private-key-file=/etc/kubernetes/ssl/service-account-key.pem --use-service-account-credentials --cluster-signing-cert-file=/etc/kubernetes/ssl/worker-ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/worker-ca-key.pem --allocate-node-cidrs=true --cluster-cidr=10.2.0.0/16 --configure-cloud-routes=false --service-cluster-ip-range=10.3.0.0/16
core      6843  0.0  0.0   2820   844 pts/0    S+   14:07   0:00 grep --colour=auto cluster-cidr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Checking the value of the &lt;code&gt;--cluster-cidr&lt;/code&gt; option:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;--cluster-cidr=10.2.0.0/16&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kubernetes grants each node a /24 CIDR blocks by default for use by the nodes Pods. Because this cluster assigns Pod IP addresses from a /16 CIDR blocks (&lt;code&gt;cluster-cidr&lt;/code&gt;), there can be up to 256 nodes (24-16 = 8, 2^8 = 256).&lt;/p&gt;
&lt;p&gt;The reason why the Pods didn&amp;rsquo;t start this time was that the HPA scaled so many Pods that there were more than 265 nodes.&lt;/p&gt;
&lt;h2 id=&#34;change-cidr-blocks-assigned-to-node&#34;&gt;Change CIDR blocks assigned to node&lt;/h2&gt;
&lt;p&gt;To solve this problem I made change to the CIDR blocks assigned to the nodes.
Changing the CIDR blocks from /24 to /25 reduces the number of Pods per nodes, but allowed will be 512 nodesï¼ˆ25-16 = 9ã€2^9 = 512).&lt;/p&gt;
&lt;p&gt;To change the CIDR blocks, run controller-manager with the following options:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;--node-cidr-mask-size=25&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Also, we limited the number of Pods that can be launched with the kubelet option to 64.
For the basis of this number 64, see the 
&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GKE document&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;--max-pods=64&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When running a large Kubernetes cluster, you need to take care of the CIDR ranges.&lt;/p&gt;
&lt;h3 id=&#34;refs&#34;&gt;Refs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optimizing IP address allocation Â |Â  Kubernetes Engine Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://srcco.de/posts/many-kubernetes-clusters.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Many Kubernetes Clusters | SRCco.de&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes Leader Election in Depth</title>
      <link>https://d-kuro.github.io/post/kubernetes-leader-election/</link>
      <pubDate>Mon, 06 Jan 2020 01:09:52 +0900</pubDate>
      <guid>https://d-kuro.github.io/post/kubernetes-leader-election/</guid>
      <description>&lt;p&gt;ã“ã®è¨˜äº‹ã§ã¯ Kubernetes ã® Leader election ã«ã¤ã„ã¦è§£èª¬ã—ã¦ã„ãã¾ã™ã€‚&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-ã«ãŠã‘ã‚‹-leader-election&#34;&gt;Kubernetes ã«ãŠã‘ã‚‹ Leader election&lt;/h2&gt;
&lt;p&gt;Kubernetes ã«ãŠã‘ã‚‹ Leader election ã¯ Controller é–“ã®ç«¶åˆã‚’é˜²ããŸã‚ã«ä½¿ã‚ã‚Œã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;ä¾‹ã¨ã—ã¦ Deployment ã‚’ watch ã—ã¦å‡¦ç†ã‚’è¡Œã† Controller ã® Pod ãŒã‚ã‚‹å ´åˆã« Replica ã® Pod ãŒ 2 å°ã ã¨ 1 ã¤ã® Deployment ã®æ“ä½œã«å¯¾ã—ã¦ 2 ã¤ã® Pod ãŒå‡¦ç†ã‚’è¡Œã†ãŸã‚, å ´åˆã«ã‚ˆã£ã¦ã¯ç«¶åˆã—ã¦ã—ã¾ã„ã¾ã™ã€‚
ãã®ã‚ˆã†ãªç«¶åˆã‚’é˜²ããŸã‚ã« Kubernetes ã§ã¯ clinet-go ã« Leader election ã®ä»•çµ„ã¿ãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚
Leader election ã‚’ä½¿ç”¨ã™ã‚‹ã¨ãƒªãƒ¼ãƒ€ãƒ¼ã¨ãªã‚‹ Controller ãŒ reconciliation loop ã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹é–“, ä»–ã® Controller ã¯å¾…æ©Ÿã—ã¾ã™ã€‚
ãƒªãƒ¼ãƒ€ãƒ¼ãŒè¾ä»»ã—ãŸå ´åˆå¾…æ©Ÿã—ã¦ã„ãŸ Controller ãŒãƒªãƒ¼ãƒ€ãƒ¼ã«æ˜‡æ ¼ã—, ã™ãã«å‡¦ç†ã‚’å†é–‹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;å¯ç”¨æ€§ãŒå¿…è¦ãª Controller ä»¥å¤–ã®å ´åˆã¯ &lt;code&gt;replicas: 1&lt;/code&gt; ã«ã™ã‚Œã°å•é¡Œãªã„, ã¨æ€ã†ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒ Deployment ã® strategy ã‚’ RollingUpdate ã«ã—ã¦ã„ã‚‹å ´åˆã¯ä¸€æ™‚çš„ã« 2 ã¤ã® Pod ãŒå‹•ä½œã™ã‚‹çŠ¶æ³ãŒå­˜åœ¨ã™ã‚‹ãŸã‚, æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬ãƒ–ãƒ­ã‚°ã§ã¯ä»¥ä¸‹ã® 2 ã¤ã® Leader election ã®å®Ÿè£…ã«ã¤ã„ã¦è§£èª¬ã—ã¦ã„ãã¾ã™ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leader-for-life
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://godoc.org/github.com/operator-framework/operator-sdk/pkg/leader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github.com/operator-framework/operator-sdk/pkg/leader&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Leader-with-lease
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://godoc.org/github.com/kubernetes-sigs/controller-runtime/pkg/leaderelection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github.com/kubernetes-sigs/controller-runtime/pkg/leaderelection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ä¸Šè¨˜ã®åå‰ã¯ Operator SDK ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŒã£ã¦ãã¦ã„ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.openshift.com/container-platform/4.1/applications/operator_sdk/osdk-leader-election.html&#34;&gt;https://docs.openshift.com/container-platform/4.1/applications/operator_sdk/osdk-leader-election.html&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;è¿½è¨˜-kubernetes-meetup-tokyo-27-ã§-lt-ã—ã¾ã—ãŸ&#34;&gt;è¿½è¨˜: Kubernetes Meetup Tokyo #27 ã§ LT ã—ã¾ã—ãŸ&lt;/h3&gt;
&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;6aa22d2c4ff34536bc52461feacb2be5&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;leader-for-life&#34;&gt;Leader-for-life&lt;/h2&gt;
&lt;p&gt;Leader-for-life ã¯ Operator SDK ãŒæä¾›ã—ã¦ã„ã‚‹ Leader election ã®ä»•çµ„ã¿ã§ã™ã€‚&lt;/p&gt;
&lt;p&gt;ä»•çµ„ã¿ã¯å˜ç´”ã§ OwnerReference ãŒãƒªãƒ¼ãƒ€ãƒ¼ã® Pod ã§ã‚ã‚‹ ConfigMap ã‚’ä½œæˆã—, ãƒ­ãƒƒã‚¯ã—ã¾ã™ã€‚
Pod ãŒå‰Šé™¤ã•ã‚Œã‚‹ã¨ Kubernetes ã®ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ä»•çµ„ã¿ã«ã‚ˆã‚Š ConfigMap ã‚‚è‡ªå‹•çš„ã«å‰Šé™¤ã•ã‚Œã‚‹ãŸã‚, ä»–ã® Pod ãŒãƒªãƒ¼ãƒ€ãƒ¼ã‚’ç²å¾—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Garbage Collection - Kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨ã®éš›ã«ã¯ä»¥ä¸‹ã®ç”¨ã« Pod ã®åå‰ã‚’ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦æ¸¡ã—ã¦ã‚ã’ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;- name: POD_NAME
  valueFrom:
    fieldRef:
      fieldPath: metadata.name
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ä½œæˆã•ã‚Œã‚‹ ConfigMap ã®ä¾‹:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2019-12-27T04:26:18Z&amp;quot;
  name: operator-lock
  namespace: default
  ownerReferences:
  - apiVersion: v1
    kind: Pod
    name: operator-787d565ff9-xzf69
    uid: fb50cbf0-2860-11ea-8930-065483f65576
  resourceVersion: &amp;quot;27146109&amp;quot;
  selfLink: /api/v1/namespaces/default/configmaps/operator-lock
  uid: 07468a04-2861-11ea-8930-065483f65576
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;åˆ©ç”¨ã™ã‚‹ãŸã‚ã®é–¢æ•°ã¯æ¬¡ã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func Become(ctx context.Context, lockName string) error
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://godoc.org/github.com/operator-framework/operator-sdk/pkg/leader#Become&#34;&gt;https://godoc.org/github.com/operator-framework/operator-sdk/pkg/leader#Become&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨ã™ã‚‹éš›ã«ã¯æ¬¡ã®ã‚ˆã†ã«ä½¿ç”¨ã—ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
  ...
  &amp;quot;github.com/operator-framework/operator-sdk/pkg/leader&amp;quot;
)

func main() {
  ...
  err = leader.Become(context.TODO(), &amp;quot;memcached-operator-lock&amp;quot;)
  if err != nil {
    log.Error(err, &amp;quot;Failed to retry for leader lock&amp;quot;)
    os.Exit(1)
  }
  ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ä»¥ä¸‹ã§ã¯è©³ã—ãã‚³ãƒ¼ãƒ‰ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚&lt;/p&gt;
&lt;h3 id=&#34;ownerreference-ã‚’ä½œæˆã™ã‚‹&#34;&gt;OwnerReference ã‚’ä½œæˆã™ã‚‹&lt;/h3&gt;
&lt;p&gt;ç’°å¢ƒå¤‰æ•° &lt;code&gt;POD_NAME&lt;/code&gt; ã‚ˆã‚Šè‡ªèº«ã® Pod ã® name ã‚’å–å¾—ã—ã¦ Pod ã‚’å–å¾—å¾Œ, ãã®æƒ…å ±ã‚’ç”¨ã„ã¦ OwnerReference ã‚’ä½œæˆã—ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;owner, err := myOwnerRef(ctx, client, ns)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/operator-framework/operator-sdk/blob/v0.13.0/pkg/leader/leader.go#L67&#34;&gt;https://github.com/operator-framework/operator-sdk/blob/v0.13.0/pkg/leader/leader.go#L67&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;configmap-ã‚’å–å¾—ã—-owner-ã‚’ç¢ºèªã™ã‚‹&#34;&gt;ConfigMap ã‚’å–å¾—ã—, Owner ã‚’ç¢ºèªã™ã‚‹&lt;/h3&gt;
&lt;p&gt;ã“ã“ã§ä¸€åº¦ ConfigMap ã‚’å–å¾—ã—ã¾ã™ã€‚
å–å¾—ã—ãŸçµæœ, è‡ªèº«ãŒ Owner ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã§ã—ãŸå ´åˆã¯ &lt;code&gt;nil&lt;/code&gt; ã‚’ return ã—é–¢æ•°ã‚’çµ‚äº†ã—ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;ãƒªãƒ¼ãƒ€ãƒ¼ã§ã‚ã‚‹ Pod ãŒå†èµ·å‹•ã•ã‚ŒãŸå ´åˆã¯ã“ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã§ãƒªãƒ¼ãƒ€ãƒ¼ã‚’ç¶™ç¶šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// check for existing lock from this pod, in case we got restarted
existing := &amp;amp;corev1.ConfigMap{}
key := crclient.ObjectKey{Namespace: ns, Name: lockName}
err = client.Get(ctx, key, existing)

switch {
case err == nil:
    for _, existingOwner := range existing.GetOwnerReferences() {
        if existingOwner.Name == owner.Name {
            log.Info(&amp;quot;Found existing lock with my name. I was likely restarted.&amp;quot;)
            log.Info(&amp;quot;Continuing as the leader.&amp;quot;)
            return nil
        }
        log.Info(&amp;quot;Found existing lock&amp;quot;, &amp;quot;LockOwner&amp;quot;, existingOwner.Name)
    }
case apierrors.IsNotFound(err):
    log.Info(&amp;quot;No pre-existing lock was found.&amp;quot;)
default:
    log.Error(err, &amp;quot;Unknown error trying to get ConfigMap&amp;quot;)
    return err
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/operator-framework/operator-sdk/blob/v0.13.0/pkg/leader/leader.go#L72-L92&#34;&gt;https://github.com/operator-framework/operator-sdk/blob/v0.13.0/pkg/leader/leader.go#L72-L92&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;configmap-ã®ä½œæˆã‚’è©¦è¡Œã™ã‚‹&#34;&gt;ConfigMap ã®ä½œæˆã‚’è©¦è¡Œã™ã‚‹&lt;/h3&gt;
&lt;p&gt;for ã§ãƒ«ãƒ¼ãƒ—ã—ãªãŒã‚‰ ConfigMap ã®ä½œæˆã‚’è©¦è¡Œã—ã¾ã™ã€‚
ã“ã®ãƒ«ãƒ¼ãƒ—ã¯ ConfigMap ãŒä½œæˆã•ã‚Œ, è‡ªèº«ãŒãƒªãƒ¼ãƒ€ãƒ¼ã«ãªã‚‹ã¾ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;ãƒªãƒ¼ãƒ€ãƒ¼ã® Pod ãŒ evict ã•ã‚ŒãŸå ´åˆã¯ãã® Pod ã®å‰Šé™¤ã‚’è¡Œã„ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// try to create a lock
backoff := time.Second
for {
    err := client.Create(ctx, cm)
    switch {
    case err == nil:
        log.Info(&amp;quot;Became the leader.&amp;quot;)
        return nil
    case apierrors.IsAlreadyExists(err):
        existingOwners := existing.GetOwnerReferences()
        switch {
        case len(existingOwners) != 1:
            log.Info(&amp;quot;Leader lock configmap must have exactly one owner reference.&amp;quot;, &amp;quot;ConfigMap&amp;quot;, existing)
        case existingOwners[0].Kind != &amp;quot;Pod&amp;quot;:
            log.Info(&amp;quot;Leader lock configmap owner reference must be a pod.&amp;quot;, &amp;quot;OwnerReference&amp;quot;, existingOwners[0])
        default:
            leaderPod := &amp;amp;corev1.Pod{}
            key = crclient.ObjectKey{Namespace: ns, Name: existingOwners[0].Name}
            err = client.Get(ctx, key, leaderPod)
            switch {
            case apierrors.IsNotFound(err):
                log.Info(&amp;quot;Leader pod has been deleted, waiting for garbage collection do remove the lock.&amp;quot;)
            case err != nil:
                return err
            case isPodEvicted(*leaderPod) &amp;amp;&amp;amp; leaderPod.GetDeletionTimestamp() == nil:
                log.Info(&amp;quot;Operator pod with leader lock has been evicted.&amp;quot;, &amp;quot;leader&amp;quot;, leaderPod.Name)
                log.Info(&amp;quot;Deleting evicted leader.&amp;quot;)
                // Pod may not delete immediately, continue with backoff
                err := client.Delete(ctx, leaderPod)
                if err != nil {
                    log.Error(err, &amp;quot;Leader pod could not be deleted.&amp;quot;)
                }

            default:
                log.Info(&amp;quot;Not the leader. Waiting.&amp;quot;)
            }
        }

        select {
        case &amp;lt;-time.After(wait.Jitter(backoff, .2)):
            if backoff &amp;lt; maxBackoffInterval {
                backoff *= 2
            }
            continue
        case &amp;lt;-ctx.Done():
            return ctx.Err()
        }
    default:
        log.Error(err, &amp;quot;Unknown error creating ConfigMap&amp;quot;)
        return err
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/operator-framework/operator-sdk/blob/v0.13.0/pkg/leader/leader.go#L102-L153&#34;&gt;https://github.com/operator-framework/operator-sdk/blob/v0.13.0/pkg/leader/leader.go#L102-L153&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;leader-with-lease&#34;&gt;Leader-with-lease&lt;/h2&gt;
&lt;p&gt;Leader-with-lease ã¯ controller-runtime ãŒæä¾›ã—ã¦ã„ã‚‹ Leader election ã®ä»•çµ„ã¿ã§ã™ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://godoc.org/github.com/kubernetes-sigs/controller-runtime/pkg/leaderelection&#34;&gt;https://godoc.org/github.com/kubernetes-sigs/controller-runtime/pkg/leaderelection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ä¸­ã§ã¯ client-go ã§æä¾›ã•ã‚Œã¦ã‚‹ &lt;code&gt;leaderelection&lt;/code&gt; package ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://godoc.org/k8s.io/client-go/tools/leaderelection&#34;&gt;https://godoc.org/k8s.io/client-go/tools/leaderelection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Leader-with-lease ã®ä»•çµ„ã¿ã¯å…ˆç¨‹ã® Leader-for-life ã¨åŒã˜ã‚ˆã†ã« ConfigMap ã¾ãŸã¯ Endpoints ã‚’ç”¨ã„ã¦åˆ†æ•£ãƒ­ãƒƒã‚¯ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
ãƒ­ãƒƒã‚¯ã‚’å–å¾—ã™ã‚‹ Controller ã¯ãƒªãƒ¼ãƒ€ãƒ¼ã«ãªã‚Š, å–å¾—ã§ããªã„ Controller ã¯å¾…æ©Ÿã—ã¾ã™ã€‚
Leader-for-life ã¨é•ã†ç‚¹ã¯ãƒªãƒ¼ã‚¹æœŸé–“ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ç‚¹ã§ã™ã€‚
ãƒªãƒ¼ãƒ€ãƒ¼ã¯å®šæœŸçš„ã«ãƒªãƒ¼ã‚¹ã‚’æ›´æ–°ã—ã¦ã„ã¾ã™ã€‚
ãƒªãƒ¼ãƒ€ãƒ¼ãŒä½•ã‚‰ã‹ã®ç†ç”±ã§æ­»ã¬ã¨ãƒªãƒ¼ã‚¹ã¯æœŸé™åˆ‡ã‚Œã«ãªã‚Š, å¾…æ©Ÿã—ã¦ã„ãŸ Controller ãŒãƒªãƒ¼ãƒ€ãƒ¼ã‚’ç²å¾—ã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L50-L52&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;ã“ã®ä»•çµ„ã¿ã¯ç¾æ™‚ç‚¹ã§ Alpha ã§ã‚ã‚Šå°†æ¥çš„ã«å¤§å¹…ã«å¤‰æ›´ã¾ãŸã¯å‰Šé™¤ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨ã™ã‚‹éš›ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ä½¿ç”¨ã—ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
  ...
  &amp;quot;sigs.k8s.io/controller-runtime/pkg/manager&amp;quot;
)

func main() {
  ...
  opts := manager.Options{
    ...
    LeaderElection: true,
    LeaderElectionID: &amp;quot;memcached-operator-lock&amp;quot;
  }
  mgr, err := manager.New(cfg, opts)
  ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ä»¥ä¸‹ã§ã¯è©³ã—ãã‚³ãƒ¼ãƒ‰ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚&lt;/p&gt;
&lt;h3 id=&#34;manager-ã®åˆæœŸåŒ–&#34;&gt;Manager ã®åˆæœŸåŒ–&lt;/h3&gt;
&lt;p&gt;ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã§è¨€ã†ã¨ä»¥ä¸‹ã®éƒ¨åˆ†ã§ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;mgr, err := manager.New(cfg, opts)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;manager.New()&lt;/code&gt; é–¢æ•°ã®ä¸­ã§, Leader election ã«é–¢ã™ã‚‹åˆæœŸåŒ–ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Create the resource lock to enable leader election)
resourceLock, err := options.newResourceLock(config, recorderProvider, leaderelection.Options{
    LeaderElection:          options.LeaderElection,
    LeaderElectionID:        options.LeaderElectionID,
    LeaderElectionNamespace: options.LeaderElectionNamespace,
})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/manager/manager.go#L265-L270&#34;&gt;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/manager/manager.go#L265-L270&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;newResourceLock&lt;/code&gt; ã¯ &lt;code&gt;setOptionsDefaults&lt;/code&gt; ã¨ã„ã†é–¢æ•°ã§åˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// setOptionsDefaults set default values for Options fields
func setOptionsDefaults(options Options) Options {
    ...
    // Allow newResourceLock to be mocked
    if options.newResourceLock == nil {
        options.newResourceLock = leaderelection.NewResourceLock
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/manager/manager.go#L375-L378&#34;&gt;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/manager/manager.go#L375-L378&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;leaderelection.NewResourceLock()&lt;/code&gt; é–¢æ•°ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚
ã“ã“ã§ client-go ã® &lt;code&gt;leaderelection&lt;/code&gt; package ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ãŒã‚ã‹ã‚Šã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
    ...
    &amp;quot;k8s.io/client-go/tools/leaderelection/resourcelock&amp;quot;
    ...
)

// NewResourceLock creates a new config map resource lock for use in a leader
// election loop
func NewResourceLock(config *rest.Config, recorderProvider recorder.Provider, options Options) (resourcelock.Interface, error) {
    // TODO(JoelSpeed): switch to leaderelection object in 1.12
    return resourcelock.New(resourcelock.ConfigMapsResourceLock,
        options.LeaderElectionNamespace,
        options.LeaderElectionID,
        client.CoreV1(),
        client.CoordinationV1(),
        resourcelock.ResourceLockConfig{
            Identity:      id,
            EventRecorder: recorderProvider.GetEventRecorderFor(id),
        })
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/leaderelection/leader_election.go#L82-L91&#34;&gt;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/leaderelection/leader_election.go#L82-L91&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;resourcelock.New()&lt;/code&gt; é–¢æ•°ã®å¼•æ•°ã« &lt;code&gt;resourcelock.ConfigMapsResourceLock&lt;/code&gt; ã¨ã„ã†å€¤ã‚’æ¸¡ã—ã¦ãŠã‚Š, controller-runtime ã§ã¯ ConfigMap ã‚’ç”¨ã„ã‚‹æ–¹æ³•ã§åˆ†æ•£ãƒ­ãƒƒã‚¯ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚&lt;/p&gt;
&lt;h3 id=&#34;leader-election-ã®å®Ÿè¡Œ&#34;&gt;Leader election ã®å®Ÿè¡Œ&lt;/h3&gt;
&lt;p&gt;Manager ã¯ &lt;code&gt;Start()&lt;/code&gt; ã¨ã„ã†é–¢æ•°ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
Leader election ã®å®Ÿè¡Œã‚‚ã“ã®ä¸­ã§è¡Œã£ã¦ã„ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (cm *controllerManager) Start(stop &amp;lt;-chan struct{}) error {
    ...
    if cm.resourceLock != nil {
        err := cm.startLeaderElection()
        if err != nil {
            return err
        }
    } else {
        go cm.startLeaderElectionRunnables()
    }
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/manager/internal.go#L424-L431&#34;&gt;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/manager/internal.go#L424-L431&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cm.startLeaderElection()&lt;/code&gt; é–¢æ•°ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚
client-go ã® &lt;code&gt;leaderelection&lt;/code&gt; package ã‚’ä½¿ç”¨ã—ã¦ Leader election ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¦ã‚‹ã®ãŒã‚ã‹ã‚Šã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
    ...
    &amp;quot;k8s.io/client-go/tools/leaderelection&amp;quot;
    &amp;quot;k8s.io/client-go/tools/leaderelection/resourcelock&amp;quot;
    ...
)

func (cm *controllerManager) startLeaderElection() (err error) {
    l, err := leaderelection.NewLeaderElector(leaderelection.LeaderElectionConfig{
        Lock:          cm.resourceLock,
        LeaseDuration: cm.leaseDuration,
        RenewDeadline: cm.renewDeadline,
        RetryPeriod:   cm.retryPeriod,
        Callbacks: leaderelection.LeaderCallbacks{
            OnStartedLeading: func(_ context.Context) {
                cm.startLeaderElectionRunnables()
            },
            OnStoppedLeading: func() {
                // Most implementations of leader election log.Fatal() here.
                // Since Start is wrapped in log.Fatal when called, we can just return
                // an error here which will cause the program to exit.
                cm.errSignal.SignalError(fmt.Errorf(&amp;quot;leader election lost&amp;quot;))
            },
        },
    })
    ...
    // Start the leader elector process
    go l.Run(ctx)
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/manager/internal.go#L510-L544&#34;&gt;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/manager/internal.go#L510-L544&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;leader-election-loop&#34;&gt;Leader election loop&lt;/h3&gt;
&lt;p&gt;1 ã¤å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§è¦‹ãŸä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ä¸­ã‚’æ˜ã‚Šä¸‹ã’ã¦ã„ãã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Start the leader elector process
go l.Run(ctx)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Run()&lt;/code&gt; é–¢æ•°ã®ä¸­ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Run starts the leader election loop
func (le *LeaderElector) Run(ctx context.Context) {
	defer func() {
		runtime.HandleCrash()
		le.config.Callbacks.OnStoppedLeading()
	}()
	if !le.acquire(ctx) {
		return // ctx signalled done
	}
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()
	go le.config.Callbacks.OnStartedLeading(ctx)
	le.renew(ctx)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L189-L202&#34;&gt;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L189-L202&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;le.acquire()&lt;/code&gt; ã¨ &lt;code&gt;le.renew()&lt;/code&gt; é–¢æ•°ã¯ã©ã¡ã‚‰ã‚‚å†…éƒ¨ã§ã¯ &lt;code&gt;le.tryAcquireOrRenew()&lt;/code&gt; é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã„ã¾ã™ã€‚
&lt;code&gt;le.acquire()&lt;/code&gt; ã§ã¯ &lt;code&gt;le.tryAcquireOrRenew()&lt;/code&gt; é–¢æ•°ãŒæˆåŠŸã—ãŸå ´åˆã¯çµ‚äº†ã—, å¤±æ•—ã—ãŸå ´åˆã¯ä¸€å®šã®å‘¨æœŸã§ãƒªãƒˆãƒ©ã‚¤ã•ã‚Œã¾ã™ã€‚
&lt;code&gt;le.renew()&lt;/code&gt; ã§ã¯ãã®é€†ã§ &lt;code&gt;le.tryAcquireOrRenew()&lt;/code&gt; é–¢æ•°ãŒå¤±æ•—ã—ãŸå ´åˆã¯çµ‚äº†ã—, æˆåŠŸã—ãŸå ´åˆã¯ä¸€å®šã®å‘¨æœŸã§ãƒªãƒˆãƒ©ã‚¤ã•ã‚Œã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;ã‚ã‹ã‚Šã‚„ã™ã„è¨€è‘‰ã§èª¬æ˜ã™ã‚‹ã¨, &lt;code&gt;le.acquire()&lt;/code&gt; ã§ã¯ãƒªãƒ¼ãƒ€ãƒ¼ã‚’ç²å¾—ã§ãã‚‹ã¾ã§ãƒªãƒˆãƒ©ã‚¤ã‚’è¡Œã„, &lt;code&gt;le.renew()&lt;/code&gt; ã§ã¯ãƒªãƒ¼ãƒ€ãƒ¼ç²å¾—ã§ãã¦ã„ã‚‹é–“ãƒªãƒˆãƒ©ã‚¤ã‚’è¡Œã„ã¾ã™ã€‚
ãƒªãƒ¼ãƒ€ãƒ¼ã‚’ãƒ­ã‚¹ãƒˆã—ãŸå ´åˆã¯ defer ã§å®Ÿè¡Œã—ã¦ã„ã‚‹ &lt;code&gt;le.config.Callbacks.OnStoppedLeading()&lt;/code&gt; ã¨ã„ã†ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã§ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã—, Controller ã‚’çµ‚äº†ã•ã›ã¾ã™ã€‚
ã“ã†ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Š Kubernetes ã® Deployment ç­‰ã®ä»•çµ„ã¿ã«ã‚ˆã‚Šæ–°ã—ã„ Pod ãŒèµ·å‹•ã—, ã¾ãŸ Leader election loop ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;Callbacks: leaderelection.LeaderCallbacks{
    OnStartedLeading: func(_ context.Context) {
        cm.startLeaderElectionRunnables()
    },
    OnStoppedLeading: func() {
        // Most implementations of leader election log.Fatal() here.
        // Since Start is wrapped in log.Fatal when called, we can just return
        // an error here which will cause the program to exit.
        cm.errSignal.SignalError(fmt.Errorf(&amp;quot;leader election lost&amp;quot;))
    },
},
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/manager/internal.go#L516-L525&#34;&gt;https://github.com/kubernetes-sigs/controller-runtime/blob/v0.4.0/pkg/manager/internal.go#L516-L525&lt;/a&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;le.acquire() Details&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// acquire loops calling tryAcquireOrRenew and returns true immediately when tryAcquireOrRenew succeeds.
// Returns false if ctx signals done.
func (le *LeaderElector) acquire(ctx context.Context) bool {
    ctx, cancel := context.WithCancel(ctx)
    defer cancel()
    succeeded := false
    desc := le.config.Lock.Describe()
    klog.Infof(&amp;quot;attempting to acquire leader lease  %v...&amp;quot;, desc)
    wait.JitterUntil(func() {
        succeeded = le.tryAcquireOrRenew()
        le.maybeReportTransition()
        if !succeeded {
            klog.V(4).Infof(&amp;quot;failed to acquire lease %v&amp;quot;, desc)
            return
        }
        le.config.Lock.RecordEvent(&amp;quot;became leader&amp;quot;)
        le.metrics.leaderOn(le.config.Name)
        klog.Infof(&amp;quot;successfully acquired lease %v&amp;quot;, desc)
        cancel()
    }, le.config.RetryPeriod, JitterFactor, true, ctx.Done())
    return succeeded
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L228-L249&#34;&gt;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L228-L249&lt;/a&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;le.renew() Details&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// renew loops calling tryAcquireOrRenew and returns immediately when tryAcquireOrRenew fails or ctx signals done.
func (le *LeaderElector) renew(ctx context.Context) {
    ctx, cancel := context.WithCancel(ctx)
    defer cancel()
    wait.Until(func() {
        timeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline)
        defer timeoutCancel()
        err := wait.PollImmediateUntil(le.config.RetryPeriod, func() (bool, error) {
            done := make(chan bool, 1)
            go func() {
                defer close(done)
                done &amp;lt;- le.tryAcquireOrRenew()
            }()

            select {
            case &amp;lt;-timeoutCtx.Done():
                return false, fmt.Errorf(&amp;quot;failed to tryAcquireOrRenew %s&amp;quot;, timeoutCtx.Err())
            case result := &amp;lt;-done:
                return result, nil
            }
        }, timeoutCtx.Done())

        le.maybeReportTransition()
        desc := le.config.Lock.Describe()
        if err == nil {
            klog.V(5).Infof(&amp;quot;successfully renewed lease %v&amp;quot;, desc)
            return
        }
        le.config.Lock.RecordEvent(&amp;quot;stopped leading&amp;quot;)
        le.metrics.leaderOff(le.config.Name)
        klog.Infof(&amp;quot;failed to renew lease %v: %v&amp;quot;, desc, err)
        cancel()
    }, le.config.RetryPeriod, ctx.Done())

    // if we hold the lease, give it up
    if le.config.ReleaseOnCancel {
        le.release()
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L251-L289&#34;&gt;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L251-L289&lt;/a&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;ä»¥ä¸‹ã§ã¯, &lt;code&gt;le.tryAcquireOrRenew()&lt;/code&gt; é–¢æ•°ã®ä¸­ã‚’è©³ã—ãè¦‹ã¦ã„ãã¾ã™ã€‚&lt;/p&gt;
&lt;h3 id=&#34;letryacquireorrenew&#34;&gt;le.tryAcquireOrRenew()&lt;/h3&gt;
&lt;details&gt;
&lt;summary&gt; le.tryAcquireOrRenew() Details&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// tryAcquireOrRenew tries to acquire a leader lease if it is not already acquired,
// else it tries to renew the lease if it has already been acquired. Returns true
// on success else returns false.
func (le *LeaderElector) tryAcquireOrRenew() bool {
    now := metav1.Now()
    leaderElectionRecord := rl.LeaderElectionRecord{
        HolderIdentity:       le.config.Lock.Identity(),
        LeaseDurationSeconds: int(le.config.LeaseDuration / time.Second),
        RenewTime:            now,
        AcquireTime:          now,
    }

    // 1. obtain or create the ElectionRecord
    oldLeaderElectionRecord, err := le.config.Lock.Get()
    if err != nil {
        if !errors.IsNotFound(err) {
            klog.Errorf(&amp;quot;error retrieving resource lock %v: %v&amp;quot;, le.config.Lock.Describe(), err)
            return false
        }
        if err = le.config.Lock.Create(leaderElectionRecord); err != nil {
            klog.Errorf(&amp;quot;error initially creating leader election record: %v&amp;quot;, err)
            return false
        }
        le.observedRecord = leaderElectionRecord
        le.observedTime = le.clock.Now()
        return true
    }

    // 2. Record obtained, check the Identity &amp;amp; Time
    if !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) {
        le.observedRecord = *oldLeaderElectionRecord
        le.observedTime = le.clock.Now()
    }
    if len(oldLeaderElectionRecord.HolderIdentity) &amp;gt; 0 &amp;amp;&amp;amp;
        le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;amp;&amp;amp;
        !le.IsLeader() {
        klog.V(4).Infof(&amp;quot;lock is held by %v and has not yet expired&amp;quot;, oldLeaderElectionRecord.HolderIdentity)
        return false
    }

    // 3. We&#39;re going to try to update. The leaderElectionRecord is set to it&#39;s default
    // here. Let&#39;s correct it before updating.
    if le.IsLeader() {
        leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime
        leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions
    } else {
        leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1
    }

    // update the lock itself
    if err = le.config.Lock.Update(leaderElectionRecord); err != nil {
        klog.Errorf(&amp;quot;Failed to update lock: %v&amp;quot;, err)
        return false
    }
    le.observedRecord = leaderElectionRecord
    le.observedTime = le.clock.Now()
    return true
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L308-L365&#34;&gt;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L308-L365&lt;/a&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;æ›´æ–°ã«ä½¿ç”¨ã™ã‚‹-leaderelectionrecord-ã®ä½œæˆ&#34;&gt;æ›´æ–°ã«ä½¿ç”¨ã™ã‚‹ LeaderElectionRecord ã®ä½œæˆ&lt;/h4&gt;
&lt;p&gt;ã¯ã˜ã‚ã«, æ›´æ–°ã«ä½¿ç”¨ã™ã‚‹ LeaderElectionRecord ã®ä½œæˆã‚’è¡Œã„ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;LeaderElectionRecord ã¨ã¯, ãƒ­ãƒƒã‚¯ã«ç”¨ã„ã‚‹ Object ã® &lt;code&gt;control-plane.alpha.kubernetes.io/leader&lt;/code&gt; annotation ã« JSON ã§æ ¼ç´ã•ã‚Œã‚‹ struct ã§ã™ã€‚
èª°ãŒãƒ­ãƒƒã‚¯ã‚’ç²å¾—ã—ã¦ã„ã‚‹ã‹ãªã©ã®æƒ…å ±ãŒæ ¼ç´ã•ã‚Œã¦ãŠã‚Š, ã“ã®æƒ…å ±ã‚’å…ƒã«ãƒ­ãƒƒã‚¯ã®åˆ¶å¾¡ã‚’è¡Œã„ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;now := metav1.Now()
leaderElectionRecord := rl.LeaderElectionRecord{
    HolderIdentity:       le.config.Lock.Identity(),
    LeaseDurationSeconds: int(le.config.LeaseDuration / time.Second),
    RenewTime:            now,
    AcquireTime:          now,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L312-L318&#34;&gt;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L312-L318&lt;/a&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;LeaderElectionRecord Details&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// LeaderElectionRecord is the record that is stored in the leader election annotation.
// This information should be used for observational purposes only and could be replaced
// with a random string (e.g. UUID) with only slight modification of this code.
// TODO(mikedanese): this should potentially be versioned
type LeaderElectionRecord struct {
    // HolderIdentity is the ID that owns the lease. If empty, no one owns this lease and
    // all callers may acquire. Versions of this library prior to Kubernetes 1.14 will not
    // attempt to acquire leases with empty identities and will wait for the full lease
    // interval to expire before attempting to reacquire. This value is set to empty when
    HolderIdentity       string      `json:&amp;quot;holderIdentity&amp;quot;`
    LeaseDurationSeconds int         `json:&amp;quot;leaseDurationSeconds&amp;quot;`
    AcquireTime          metav1.Time `json:&amp;quot;acquireTime&amp;quot;`
    RenewTime            metav1.Time `json:&amp;quot;renewTime&amp;quot;`
    LeaderTransitions    int         `json:&amp;quot;leaderTransitions&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/resourcelock/interface.go#L35-L50&#34;&gt;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/resourcelock/interface.go#L35-L50&lt;/a&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h4 id=&#34;leaderelectionrecord-ã®å–å¾—&#34;&gt;LeaderElectionRecord ã®å–å¾—&lt;/h4&gt;
&lt;p&gt;LeaderElectionRecord ã®å–å¾— or ä½œæˆã‚’è¡Œã„ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// 1. obtain or create the ElectionRecord
oldLeaderElectionRecord, err := le.config.Lock.Get()
if err != nil {
    if !errors.IsNotFound(err) {
        klog.Errorf(&amp;quot;error retrieving resource lock %v: %v&amp;quot;, le.config.Lock.Describe(), err)
        return false
    }
    if err = le.config.Lock.Create(leaderElectionRecord); err != nil {
        klog.Errorf(&amp;quot;error initially creating leader election record: %v&amp;quot;, err)
        return false
    }
    le.observedRecord = leaderElectionRecord
    le.observedTime = le.clock.Now()
    return true
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L320-L334&#34;&gt;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L320-L334&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;ãƒ­ãƒƒã‚¯ã®æ‰€æœ‰è€…ã¨ãƒªãƒ¼ã‚¹æ™‚é–“ã®ç¢ºèª&#34;&gt;ãƒ­ãƒƒã‚¯ã®æ‰€æœ‰è€…ã¨ãƒªãƒ¼ã‚¹æ™‚é–“ã®ç¢ºèª&lt;/h4&gt;
&lt;p&gt;å–å¾—ã—ãŸ LeaderElectionRecord ã®æƒ…å ±ã‚’å–ã‚Šè¾¼ã¿, ãƒ­ãƒƒã‚¯ã®æ‰€æœ‰è€…ã¨æ™‚é–“ã®ç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;ç¾åœ¨ã®ãƒ­ãƒƒã‚¯ã‚’ä¿æŒã—ã¦ã„ã‚‹ã®ãŒè‡ªèº«ã§ã¯ãªã, æœ€å¾Œã«ç¢ºèªã—ãŸæ™‚é–“ãŒãƒªãƒ¼ã‚¹æ™‚é–“ã‚’è¶…ãˆã¦ã„ãªã„å ´åˆã¯, ç¾åœ¨ã®ãƒ­ãƒƒã‚¯ã¯ä»–äººã«ç¢ºä¿ã•ã‚Œã¦ã„ã‚‹ã¨ã¿ãªã—, &lt;code&gt;false&lt;/code&gt; ã‚’ return ã—ã¾ã™ã€‚
ã“ã‚Œã‚ˆã‚Šä¸‹ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯, è‡ªèº«ãŒãƒªãƒ¼ãƒ€ãƒ¼ã§ã‚ã‚‹å ´åˆã‹ãƒªãƒ¼ãƒ€ãƒ¼ã®å…¥ã‚Œæ›¿ãˆã‚’è¡Œã†éš›ã«é€šéã—ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// 2. Record obtained, check the Identity &amp;amp; Time
if !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) {
    le.observedRecord = *oldLeaderElectionRecord
    le.observedTime = le.clock.Now()
}
if len(oldLeaderElectionRecord.HolderIdentity) &amp;gt; 0 &amp;amp;&amp;amp;
    le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;amp;&amp;amp;
    !le.IsLeader() {
    klog.V(4).Infof(&amp;quot;lock is held by %v and has not yet expired&amp;quot;, oldLeaderElectionRecord.HolderIdentity)
    return false
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L336-L346&#34;&gt;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L336-L346&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;leaderelectionrecord-ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ›´æ–°&#34;&gt;LeaderElectionRecord ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ›´æ–°&lt;/h4&gt;
&lt;p&gt;æ›´æ–°ç”¨ã® LeaderElectionRecord ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ›´æ–°ã—ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;è‡ªèº«ãŒãƒªãƒ¼ãƒ€ãƒ¼ã§ã‚ã‚‹å ´åˆã¯ &lt;code&gt;AcquireTime&lt;/code&gt; ã«ã¯å¤ã„å€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
ãƒªãƒ¼ãƒ€ãƒ¼ã®å…¥ã‚Œæ›¿ãˆæ™‚ã«ã¯ &lt;code&gt;LeaderTransitions&lt;/code&gt; ã« 1 ã‚’åŠ ç®—ã—ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// 3. We&#39;re going to try to update. The leaderElectionRecord is set to it&#39;s default
// here. Let&#39;s correct it before updating.
if le.IsLeader() {
    leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime
    leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions
} else {
    leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L348-L355&#34;&gt;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L348-L355&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;ãƒ­ãƒƒã‚¯æƒ…å ±ã®æ›´æ–°&#34;&gt;ãƒ­ãƒƒã‚¯æƒ…å ±ã®æ›´æ–°&lt;/h4&gt;
&lt;p&gt;ãƒ­ãƒƒã‚¯ã®æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;ãƒ­ãƒƒã‚¯ã«ã¯ Kubernetes ã® Object ã‚’ç”¨ã„ã¦ã„ã‚‹ãŸã‚ Kubernetes API ã® Atomicity ã®ä»•çµ„ã¿ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
Kubernetes ã® API Server ã¯ Object ã® æ›´æ–°æ™‚ã«æŒ‡å®šã•ã‚ŒãŸ &lt;code&gt;resourceVersion&lt;/code&gt; ã¨ç¾åœ¨ã® &lt;code&gt;resourceVersion&lt;/code&gt; ãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—, æ›´æ–°æ“ä½œã®é–“ã«ä»–ã®æ›´æ–°ãŒè¡Œã‚ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes API Concepts #Resource Versions - Kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// update the lock itself
if err = le.config.Lock.Update(leaderElectionRecord); err != nil {
    klog.Errorf(&amp;quot;Failed to update lock: %v&amp;quot;, err)
    return false
}
le.observedRecord = leaderElectionRecord
le.observedTime = le.clock.Now()
return true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L357-L364&#34;&gt;https://github.com/kubernetes/client-go/blob/v12.0.0/tools/leaderelection/leaderelection.go#L357-L364&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://docs.openshift.com/container-platform/4.1/applications/operator_sdk/osdk-leader-election.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Configuring leader election - Operator SDK | Applications | OpenShift Container Platform 4.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.2/html/operators/osdk-leader-election&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;11.6. ãƒªãƒ¼ãƒ€ãƒ¼é¸æŠã®è¨­å®š 4.2 | Red Hat Customer Portal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://blog.fatedier.com/2019/04/17/k8s-custom-controller-high-available/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes è‡ªå®šä¹‰æ§åˆ¶å™¨çš„é«˜å¯ç”¨&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Deployment ã® API Version ãŒå¤ã„ã¨ revisionHistoryLimit ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒ 2147483647 ã«è¨­å®šã•ã‚Œã‚‹</title>
      <link>https://d-kuro.github.io/post/deployment-revisionhistorylimit-default/</link>
      <pubDate>Tue, 17 Dec 2019 23:45:16 +0900</pubDate>
      <guid>https://d-kuro.github.io/post/deployment-revisionhistorylimit-default/</guid>
      <description>&lt;p&gt;å°ãƒã‚¿ã§ã™ã€‚&lt;/p&gt;
&lt;p&gt;ã‚ã‚‹æ—¥ &lt;code&gt;kubectl get replicaset&lt;/code&gt; ã¨ã‹ã‚’å©ã„ãŸéš›ã«ã‚„ãŸã‚‰å±¥æ­´ã®æ•°ãŒå¤šã„ã“ã¨ã«æ°—ã¥ãã¾ã—ãŸã€‚&lt;/p&gt;
&lt;p&gt;Deployment ã«ç´ã¥ã ReplicaSet ã®å±¥æ­´ã®ä¿æŒæ•°ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦ &lt;code&gt;revisionHistoryLimit&lt;/code&gt; ã¨ã„ã†ã‚‚ã®ãŒå­˜åœ¨ã—ã¾ã™ã€‚
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯ç¢ºã‹ 10 ã ã£ãŸã¯ãšã¨å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ã«è¡Œãã¾ã—ãŸãŒã€ãã“ã«ã¯&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can set &lt;code&gt;.spec.revisionHistoryLimit&lt;/code&gt; field in a Deployment to specify how many old ReplicaSets for this Deployment you want to retain. The rest will be garbage-collected in the background. By default, it is 10.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#clean-up-policy&#34;&gt;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#clean-up-policy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ã¨è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;ã§ã‚‚å®Ÿéš›ã« &lt;code&gt;kubectl get deployment foo -o yaml&lt;/code&gt; ã‚’ã—ã¦ã¿ã‚‹ã¨å®Ÿéš›ã«è¨­å®šã•ã‚Œã¦ã„ãŸã®ã¯ &lt;code&gt;revisionHistoryLimit: 2147483647&lt;/code&gt; ã§ã—ãŸã€‚
ã†ãƒ¼ã‚“å¤šã„ã€‚ã“ã†ã„ã†ã¨ãã®ä¾¿åˆ©ã‚³ãƒãƒ³ãƒ‰ã¨ã—ã¦ &lt;code&gt;kubectl explain&lt;/code&gt; ã¨ã„ã†ã‚³ãƒãƒ³ãƒ‰ãŒå­˜åœ¨ã—ã¾ã™ã€‚&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl explain deployment.spec.revisionHistoryLimit&lt;/code&gt; ã®ã‚ˆã†ãªæ„Ÿã˜ã§å„ Object ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;$ kubectl explain deployment.spec.revisionHistoryLimit
KIND:     Deployment
VERSION:  extensions/v1beta1

FIELD:    revisionHistoryLimit &amp;lt;integer&amp;gt;

DESCRIPTION:
     The number of old ReplicaSets to retain to allow rollback. This is a
     pointer to distinguish between explicit zero and not specified. This is set
     to the max value of int32 (i.e. 2147483647) by default, which means
     &amp;quot;retaining all old RelicaSets&amp;quot;.
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;This is set to the max value of int32 (i.e. 2147483647) by default, which means &amp;ldquo;retaining all old RelicaSets&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;2147483647&lt;/code&gt; ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã®ã¯æ­£ã—ãã†ã§ã™ãŒã€å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¨˜è¼‰ã¨ã¯é£Ÿã„é•ã„ã¾ã™ã€‚
ã“ã“ã§æ³¨ç›®ã—ã¦ã»ã—ã„ã®ãŒ &lt;code&gt;VERSION:  extensions/v1beta1&lt;/code&gt; ã§ã™ã€‚å¤ã„ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;$ kubectl explain -h
List the fields for supported resources

 This command describes the fields associated with each supported API resource. Fields are identified via a simple
JSONPath identifier:

  &amp;lt;type&amp;gt;.&amp;lt;fieldName&amp;gt;[.&amp;lt;fieldName&amp;gt;]

 Add the --recursive flag to display all of the fields at once without descriptions. Information about each field is
retrieved from the server in OpenAPI format.

Use &amp;quot;kubectl api-resources&amp;quot; for a complete list of supported resources.

Examples:
  # Get the documentation of the resource and its fields
  kubectl explain pods

  # Get the documentation of a specific field of a resource
  kubectl explain pods.spec.containers

Options:
      --api-version=&#39;&#39;: Get different explanations for particular API version
      --recursive=false: Print the fields of fields (Currently only 1 level deep)

Usage:
  kubectl explain RESOURCE [options]

Use &amp;quot;kubectl options&amp;quot; for a list of global command-line options (applies to all commands).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Help ã‚’è¦‹ã‚‹ã¨ &lt;code&gt;--api-version&lt;/code&gt; ã¨ã„ã†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æŒ‡å®šãŒã§ããã†ã§ã™ã€‚
&lt;code&gt;apps/v1&lt;/code&gt; ã®æ–¹ã‚’ explain ã—ã¦ã¿ã¾ã™ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;$ kubectl explain deployment.spec.revisionHistoryLimit --api-version apps/v1
KIND:     Deployment
VERSION:  apps/v1

FIELD:    revisionHistoryLimit &amp;lt;integer&amp;gt;

DESCRIPTION:
     The number of old ReplicaSets to retain to allow rollback. This is a
     pointer to distinguish between explicit zero and not specified. Defaults to
     10.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ã¯ã„ã€å®Œå…¨ã«ç†è§£ã—ã¾ã—ãŸã€‚
ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯è¿½å¾“ã—ã¾ã—ã‚‡ã†ã€‚
Deployment ã® API Version ãŒ &lt;code&gt;apps/v1beta2&lt;/code&gt; ã«ãªã£ãŸéš›ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãŒ 10 ã«ãªã£ãŸã¿ãŸã„ã§ã™ã€‚&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl explain&lt;/code&gt; ã¯æ„å¤–ã¨ä¾¿åˆ©ãªã®ã§ãŠã™ã™ã‚ã§ã™ã€‚ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯è¿½å¾“ã—ã¾ã—ã‚‡ã† (å¤§äº‹ãªã“ã¨ãªã®ã§äºŒå›è¨€ã„ã¾ã—ãŸ)&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
